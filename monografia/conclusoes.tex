\chapter{\textbf{CONCLUSÃO}}

Este trabalho teve como objetivo desenvolver uma ferramenta baseada em inteligência artificial, utilizando o algoritmo YOLOv11, capaz de reconhecer objetos e elementos urbanos no campus Pampulha da UFMG, com o intuito de oferecer suporte à locomoção autônoma de pessoas com deficiência visual. A proposta buscou contribuir para a inclusão no ambiente universitário por meio do uso de tecnologias assistivas que possam promover maior autonomia e segurança.

Para atingir esse propósito, foi construída uma base de dados personalizada composta por imagens extraídas de vídeos gravados no próprio campus, considerando diferentes horários, condições de luminosidade e variações de ocupação dos espaços. As imagens foram anotadas manualmente e processadas utilizando técnicas de \textit{data augmentation}, com o objetivo de expandir a diversidade dos exemplos e garantir robustez ao modelo. O processo de treinamento foi realizado por meio do ambiente Google Colab, utilizando o modelo yolo11m.pt, com base em pesos pré-treinados e um conjunto de hiperparâmetros ajustado automaticamente pela biblioteca Ultralytics.

Os resultados quantitativos demonstraram que o modelo atingiu um desempenho robusto, com mAP@0.5 de 0.967 e mAP@0.5:0.95 de 0.670, o que indica boa capacidade tanto de classificação quanto de localização dos objetos de interesse. As métricas individuais por classe também se mostraram expressivas, especialmente para placa de ônibus e faixa de pedestre, com valores de \textit{precision} e \textit{recall} superiores a 0.970. A classe banco, embora com desempenho satisfatório, apresentou maior incidência de falsos positivos, sugerindo margem para aprimoramentos futuros no balanceamento do \textit{dataset} e na calibragem de limiares de confiança.

A análise qualitativa das detecções reforçou a eficácia prática do sistema, evidenciando sua capacidade de operar em diferentes cenários visuais, mesmo sob variações de iluminação. As limitações identificadas, como a maior confusão entre bancos e o \textit{background} em certas condições, bem como a ausência de testes com usuários reais e o funcionamento apenas com vídeos gravados (não em tempo real), não comprometem a viabilidade do projeto, mas delimitam seu escopo como uma prova de conceito funcional.

Embora não tenha sido possível implementar a aplicação em tempo real dentro do escopo deste trabalho, os resultados obtidos apontam para o potencial de aplicação futura dessa tecnologia em dispositivos móveis com \textit{background} auditivo automatizado. A ferramenta desenvolvida mostra-se promissora para auxiliar pessoas com deficiência visual na navegação em ambientes públicos complexos como o campus universitário, contribuindo com os princípios de acessibilidade e inclusão.

Como sugestões para trabalhos futuros, destaca-se a possibilidade de expandir o número de classes de objetos detectados, realizar testes em tempo real com câmeras acopladas a dispositivos móveis, integrar a ferramenta com sistemas de \textit{background} por vibração ou áudio adaptativo, e sobretudo, conduzir testes com usuários reais, o que poderá fornecer insights mais profundos sobre a usabilidade, eficiência e impacto social da ferramenta proposta.

Dessa forma, este trabalho não apenas cumpre seus objetivos iniciais, como estabelece uma base sólida para o desenvolvimento contínuo de soluções tecnológicas voltadas à acessibilidade, aliando inovação técnica e compromisso com a inclusão social no espaço universitário.